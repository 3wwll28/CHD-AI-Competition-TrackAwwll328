# ops.py - ç¡®ä¿å¯¼å…¥æ­£ç¡®
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.init import xavier_uniform_, constant_

# ä» dimension_adapter å¯¼å…¥å¿…è¦çš„ç»„ä»¶
from dimension_adapter import create_safe_cross_attention

class MSDeformAttn(nn.Module):
    """
    å¤šå°ºåº¦å¯å˜å½¢æ³¨æ„åŠ›æœºåˆ¶ - å®Œå…¨ä¿®å¤ç‰ˆæœ¬
    """
    def __init__(self, d_model=256, n_levels=4, n_heads=8, n_points=4):
        super().__init__()
        
        if d_model % n_heads != 0:
            raise ValueError(f"d_model must be divisible by n_heads, but got {d_model} and {n_heads}")
        
        self.d_model = d_model
        self.n_levels = n_levels
        self.n_heads = n_heads
        self.n_points = n_points
        
        # ä½¿ç”¨å®‰å…¨çš„æ³¨æ„åŠ›å®ç°
        self.attention = create_safe_cross_attention(d_model, n_heads)
        
        # ä¿æŒåŸæœ‰æ¥å£ä½†å®é™…ä½¿ç”¨å®‰å…¨æ³¨æ„åŠ›
        self.sampling_offsets = nn.Linear(d_model, n_heads * n_levels * n_points * 2)
        self.attention_weights = nn.Linear(d_model, n_heads * n_levels * n_points)
        self.value_proj = nn.Linear(d_model, d_model)
        self.output_proj = nn.Linear(d_model, d_model)
        
        self._reset_parameters()
        
    def _reset_parameters(self):
        """å‚æ•°åˆå§‹åŒ–"""
        # ç®€åŒ–çš„åˆå§‹åŒ–
        constant_(self.sampling_offsets.weight.data, 0.)
        constant_(self.sampling_offsets.bias.data, 0.)
        constant_(self.attention_weights.weight.data, 0.)
        constant_(self.attention_weights.bias.data, 0.)
        xavier_uniform_(self.value_proj.weight.data)
        constant_(self.value_proj.bias.data, 0.)
        xavier_uniform_(self.output_proj.weight.data)
        constant_(self.output_proj.bias.data, 0.)
        
    def forward(self, query, reference_points, value, spatial_shapes, level_start_index, padding_mask=None):
        """
        å‰å‘ä¼ æ’­ - ä½¿ç”¨å®‰å…¨æ³¨æ„åŠ›
        """
        print(f"      ğŸ”§ å¯å˜å½¢æ³¨æ„åŠ›: query={query.shape}, value={value.shape}")
        
        try:
            # ä½¿ç”¨å®‰å…¨çš„å¤šå¤´æ³¨æ„åŠ›
            output, _ = self.attention(
                query=query,
                key=value,
                value=value,
                key_padding_mask=padding_mask
            )
            
        except Exception as e:
            print(f"      âš ï¸ å®‰å…¨æ³¨æ„åŠ›å¤±è´¥: {e}")
            # æœ€ç®€å•çš„å¤‡ç”¨æ–¹æ¡ˆ
            output = query
        
        print(f"      âœ… å¯å˜å½¢æ³¨æ„åŠ›è¾“å‡º: {output.shape}")
        return output

# å¯¼å‡ºç±»
__all__ = ['MSDeformAttn']
# text_processor.py - çº¯æ‰©å¤§è¯å…¸ç‰ˆæœ¬ï¼ˆæ— BERTï¼‰
import torch
import torch.nn as nn
from typing import Dict, List, Tuple
import math

class EnhancedTokenizer:
    """å¢å¼ºç‰ˆè¯æ±‡è¡¨ï¼Œä¸“é—¨ä¸º3Dè§†è§‰è¯­è¨€è·Ÿè¸ªæ¯”èµ›ä¼˜åŒ–"""
    
    def __init__(self):
        self.vocab = self._build_enhanced_vocab()
        self.unk_token_id = 1
        self.pad_token_id = 0
        self.cls_token_id = 2
        self.sep_token_id = 3
        
    def _build_enhanced_vocab(self):
        """æ„å»ºä¸“é—¨ä¸ºæ¯”èµ›ä¼˜åŒ–çš„è¯æ±‡è¡¨"""
        base_words = [
            # ç‰¹æ®Šæ ‡è®°
            '[PAD]', '[UNK]', '[CLS]', '[SEP]',
            
            # æ ¸å¿ƒè·Ÿè¸ªåŠ¨è¯ï¼ˆæ¯”èµ›å…³é”®ï¼‰
            'track', 'follow', 'find', 'locate', 'detect', 'identify', 'monitor',
            'trace', 'watch', 'observe', 'search', 'seek',
            
            # é¢œè‰²æè¿°ï¼ˆä»ç¤ºä¾‹æ•°æ®ä¸­æå–ï¼‰
            'white', 'black', 'red', 'blue', 'green', 'yellow', 'silver', 'grey',
            'gray', 'orange', 'purple', 'brown', 'pink', 'dark', 'light',
            
            # è½¦è¾†ç±»å‹ï¼ˆä»ç¤ºä¾‹æ•°æ®ä¸­æå–ï¼‰
            'car', 'vehicle', 'van', 'truck', 'bus', 'motorcycle', 'bicycle',
            'suv', 'sedan', 'minivan', 'pickup', 'ambulance', 'police', 'taxi',
            'jeep', 'limousine', 'tractor', 'trailer',
            
            # è½¦è¾†çŠ¶æ€ï¼ˆä»ç¤ºä¾‹æ•°æ®ä¸­æå–ï¼‰
            'moving', 'parking', 'stopped', 'running', 'driving', 'waiting',
            'accelerating', 'braking', 'turning', 'reversing', 'passing',
            'standing', 'stationary',
            
            # ç©ºé—´ä½ç½®å’Œæ–¹å‘
            'left', 'right', 'front', 'back', 'rear', 'middle', 'side', 
            'center', 'top', 'bottom', 'upper', 'lower', 'corner', 'edge',
            'near', 'far', 'close', 'distant', 'adjacent', 'beside',
            'above', 'below', 'under', 'over', 'behind', 'ahead',
            
            # ç›¸å¯¹ä½ç½®æè¿°
            'relative', 'position', 'location', 'orientation', 'direction',
            'facing', 'pointing', 'heading',
            
            # é“è·¯å’Œåœºæ™¯å…ƒç´ 
            'road', 'street', 'highway', 'intersection', 'crosswalk', 'sidewalk',
            'parking', 'lot', 'garage', 'bridge', 'tunnel', 'roundabout',
            'lane', 'curb', 'shoulder',
            
            # äººç‰©ç±»å‹
            'person', 'pedestrian', 'cyclist', 'driver', 'rider', 'passenger',
            'walker', 'jogger', 'runner',
            
            # æ—¶é—´å’Œé¡ºåº
            'first', 'second', 'third', 'last', 'next', 'previous', 'current',
            'initial', 'final', 'beginning', 'end', 'start', 'finish',
            'before', 'after', 'during', 'while',
            
            # å°ºå¯¸å’Œå½¢çŠ¶
            'small', 'large', 'big', 'tiny', 'huge', 'long', 'short', 'tall',
            'wide', 'narrow', 'heavy', 'light', 'size', 'dimension',
            'height', 'width', 'length', 'depth',
            
            # è¿åŠ¨æè¿°
            'speed', 'velocity', 'fast', 'slow', 'quickly', 'slowly',
            'suddenly', 'gradually', 'steady', 'constant',
            'straight', 'curved', 'zigzag', 'circular',
            
            # è§†è§‰ç‰¹å¾
            'visible', 'invisible', 'occluded', 'truncated', 'clear', 'obscured',
            'bright', 'dark', 'shadow', 'reflection',
            
            # å¸¸è§ä»‹è¯å’Œè¿æ¥è¯
            'the', 'a', 'an', 'in', 'on', 'at', 'by', 'with', 'from', 'to',
            'and', 'or', 'but', 'while', 'when', 'where', 'which', 'that',
            'of', 'for', 'as', 'like',
            
            # æ•°å­—ï¼ˆ0-99ï¼‰
            *[str(i) for i in range(100)],
            
            # å¸¸è§é‡è¯
            'meter', 'meters', 'foot', 'feet', 'degree', 'degrees',
            'pixel', 'pixels', 'frame', 'frames',
            
            # ä»ç¤ºä¾‹æè¿°ä¸­æå–çš„å…³é”®è¯
            'distinctively', 'measuring', 'observed', 'starting', 'viewer',
            'position', 'azimuth', 'ranks', 'initially', 'unobstructed',
            'facing', 'positioned', 'similarly', 'continues', 'slightly',
            'closer', 'partially', 'truncated', 'towards', 'maintaining',
            'orientation', 'remains', 'nearest', 'direction',
        ]
        
        # åˆ›å»ºè¯æ±‡è¡¨
        vocab = {word: idx for idx, word in enumerate(base_words)}
        
        print(f"ğŸ“š æ¯”èµ›ä¸“ç”¨è¯æ±‡è¡¨æ„å»ºå®Œæˆï¼Œå…± {len(vocab)} ä¸ªè¯æ¡")
        return vocab
    
    def tokenize(self, text: str) -> List[str]:
        """æ”¹è¿›çš„åˆ†è¯æ–¹æ³•ï¼Œæ”¯æŒå¤åˆè¯è¯†åˆ«"""
        text = text.lower().strip()
        
        # ç‰¹æ®Šå¤„ç†ï¼šæ›¿æ¢å¸¸è§å˜ä½“
        text = text.replace("'s", " 's")
        
        tokens = []
        words = text.split()
        
        i = 0
        while i < len(words):
            matched = False
            
            # ä¼˜å…ˆæ£€æŸ¥3è¯ç»„åˆï¼ˆå¦‚ "lower right corner"ï¼‰
            if i + 2 < len(words):
                compound = words[i] + ' ' + words[i+1] + ' ' + words[i+2]
                if compound in self.vocab:
                    tokens.append(compound)
                    i += 3
                    matched = True
                    continue
            
            # æ£€æŸ¥2è¯ç»„åˆï¼ˆå¦‚ "white car", "moving from"ï¼‰
            if i + 1 < len(words) and not matched:
                compound = words[i] + ' ' + words[i+1]
                if compound in self.vocab:
                    tokens.append(compound)
                    i += 2
                    matched = True
                    continue
            
            # å•å­—è¯
            if not matched:
                # æ¸…ç†å•è¯ï¼ˆç§»é™¤æ ‡ç‚¹ï¼‰
                word = words[i].strip('.,!?;:"\'()[]{}')
                if word:  # ç¡®ä¿ä¸æ˜¯ç©ºå­—ç¬¦ä¸²
                    tokens.append(word)
                i += 1
        
        return tokens
    
    def convert_tokens_to_ids(self, tokens: List[str]) -> List[int]:
        return [self.vocab.get(token, self.unk_token_id) for token in tokens]
    
    def __call__(self, text: str, max_length: int = 128) -> Dict[str, torch.Tensor]:
        tokens = self.tokenize(text)
        tokens = ['[CLS]'] + tokens + ['[SEP]']
        
        input_ids = self.convert_tokens_to_ids(tokens)
        
        # å¡«å……æˆ–æˆªæ–­
        if len(input_ids) > max_length:
            input_ids = input_ids[:max_length-1] + [self.sep_token_id]
        else:
            input_ids = input_ids + [self.pad_token_id] * (max_length - len(input_ids))
        
        attention_mask = [1 if token_id != self.pad_token_id else 0 for token_id in input_ids]
        
        return {
            'input_ids': torch.tensor([input_ids], dtype=torch.long),
            'attention_mask': torch.tensor([attention_mask], dtype=torch.long)
        }

class SimpleTextEncoder(nn.Module):
    """ç®€å•çš„æ–‡æœ¬ç¼–ç å™¨ - ä»…ä½¿ç”¨åµŒå…¥å±‚å’Œæ± åŒ–"""
    
    def __init__(self, vocab_size, hidden_dim=256, max_seq_length=128):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.max_seq_length = max_seq_length
        
        # è¯åµŒå…¥å±‚
        self.word_embeddings = nn.Embedding(vocab_size, hidden_dim, padding_idx=0)
        
        # ä½ç½®ç¼–ç 
        self.position_embeddings = nn.Embedding(max_seq_length, hidden_dim)
        
        # å±‚å½’ä¸€åŒ–
        self.layer_norm = nn.LayerNorm(hidden_dim)
        
        # ç®€å•çš„æŠ•å½±å±‚
        self.projection = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 2),
            nn.ReLU(),
            nn.Linear(hidden_dim * 2, hidden_dim)
        )
        
        # åˆå§‹åŒ–
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        nn.init.normal_(self.word_embeddings.weight, mean=0.0, std=0.02)
        nn.init.normal_(self.position_embeddings.weight, mean=0.0, std=0.02)
    
    def forward(self, input_ids, attention_mask=None):
        batch_size, seq_length = input_ids.shape
        
        # è¯åµŒå…¥
        word_embeddings = self.word_embeddings(input_ids)
        
        # ä½ç½®ç¼–ç 
        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)
        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)
        position_embeddings = self.position_embeddings(position_ids)
        
        # ç»„åˆåµŒå…¥
        embeddings = word_embeddings + position_embeddings
        embeddings = self.layer_norm(embeddings)
        
        # åº”ç”¨æ³¨æ„åŠ›æ©ç 
        if attention_mask is not None:
            attention_mask = attention_mask.unsqueeze(-1)
            embeddings = embeddings * attention_mask
        
        # å¹³å‡æ± åŒ–
        if attention_mask is not None:
            sum_embeddings = torch.sum(embeddings * attention_mask, dim=1)
            sum_mask = torch.sum(attention_mask, dim=1)
            pooled_output = sum_embeddings / (sum_mask + 1e-9)
        else:
            pooled_output = torch.mean(embeddings, dim=1)
        
        # æŠ•å½±åˆ°ç›®æ ‡ç»´åº¦
        text_features = self.projection(pooled_output)
        
        return text_features.unsqueeze(1)  # [batch_size, 1, hidden_dim]

class TextProcessor:
    """
    æ–‡æœ¬å¤„ç†å™¨ - çº¯æ‰©å¤§è¯å…¸ç‰ˆæœ¬
    è¾“å‡ºæ ¼å¼: text_features [1, 1, hidden_dim], text_mask [1, seq_len]
    """
    
    def __init__(self, hidden_dim=256, device=None):
        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')
        self.hidden_dim = hidden_dim
        
        print(f"ğŸ¯ åˆå§‹åŒ–æ–‡æœ¬å¤„ç†å™¨ï¼ˆçº¯è¯å…¸ç‰ˆæœ¬ï¼‰ï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–å¢å¼ºtokenizer
        self.tokenizer = EnhancedTokenizer()
        
        # æ„å»ºç®€å•çš„æ–‡æœ¬ç¼–ç å™¨
        vocab_size = len(self.tokenizer.vocab)
        self.model = SimpleTextEncoder(vocab_size, hidden_dim).to(self.device)
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
        
        print(f"âœ… æ–‡æœ¬å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ! è¯æ±‡è¡¨å¤§å°: {vocab_size}, éšè—ç»´åº¦: {hidden_dim}")
    
    def encode_text(self, text: str):
        """
        ç¼–ç æ–‡æœ¬
        è¿”å›: DictåŒ…å« 'features' å’Œ 'mask'
        """
        with torch.no_grad():
            # ä½¿ç”¨å¢å¼ºtokenizer
            inputs = self.tokenizer(text)
            input_ids = inputs['input_ids'].to(self.device)
            attention_mask = inputs['attention_mask'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            text_features = self.model(input_ids, attention_mask)
            
            print(f"ğŸ“ æ–‡æœ¬ç‰¹å¾æå–å®Œæˆ:")
            print(f"   è¾“å…¥: '{text}'")
            print(f"   Tokenæ•°é‡: {attention_mask.sum().item()}")
            print(f"   ç‰¹å¾å½¢çŠ¶: {text_features.shape}")
            print(f"   æ©ç å½¢çŠ¶: {attention_mask.shape}")
            
            return {
                'features': text_features,  # [1, 1, hidden_dim]
                'mask': attention_mask      # [1, seq_len]
            }
    
    def __call__(self, text_query):
        """ä¿æŒä¸åŸä»£ç ç›¸åŒçš„è°ƒç”¨æ–¹å¼"""
        return self.encode_text(text_query)

# æµ‹è¯•å‡½æ•°
def test_enhanced_processor():
    """æµ‹è¯•å¢å¼ºæ–‡æœ¬å¤„ç†å™¨"""
    print("ğŸ§ª æµ‹è¯•å¢å¼ºæ–‡æœ¬å¤„ç†å™¨ï¼ˆçº¯è¯å…¸ç‰ˆæœ¬ï¼‰")
    print("=" * 50)
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = TextProcessor(hidden_dim=256)
    
    # æµ‹è¯•æ¯”èµ›ç›¸å…³çš„æŸ¥è¯¢
    test_queries = [
        "track the white bus moving from lower right corner",
        "find the red car parking in the middle",
        "follow the black van turning left at intersection",
        "locate the silver vehicle facing front",
        "track white bus 2.5 meters height 7.3 meters length"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢: '{query}'")
        result = processor.encode_text(query)
        
        print(f"âœ… è¾“å‡ºæ ¼å¼:")
        print(f"   features å½¢çŠ¶: {result['features'].shape}")
        print(f"   mask å½¢çŠ¶: {result['mask'].shape}")
        
        # ç»Ÿè®¡æœªçŸ¥è¯
        input_ids = processor.tokenizer(query)['input_ids'][0]
        unk_count = (input_ids == processor.tokenizer.unk_token_id).sum().item()
        print(f"   æœªçŸ¥è¯æ•°é‡: {unk_count}")
        
        # éªŒè¯ç»´åº¦
        assert result['features'].shape[2] == 256, f"ç‰¹å¾ç»´åº¦åº”è¯¥æ˜¯256ï¼Œä½†å¾—åˆ°{result['features'].shape[2]}"
        assert result['features'].shape[0] == 1, "batchç»´åº¦åº”è¯¥æ˜¯1"

if __name__ == "__main__":
    test_enhanced_processor()
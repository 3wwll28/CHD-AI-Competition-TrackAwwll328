# position_encoder.py - åˆ›å»ºæ–°æ–‡ä»¶ä¸“é—¨å¤„ç†ä½ç½®ç¼–ç 
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class SafePositionEncoder:
    """å®‰å…¨ä½ç½®ç¼–ç å™¨ï¼Œé¿å…ç»´åº¦ä¸åŒ¹é…"""
    
    @staticmethod
    def create_2d_positional_encoding(height, width, channels, device):
        """åˆ›å»º2Dä½ç½®ç¼–ç  - å®‰å…¨ç‰ˆæœ¬"""
        try:
            print(f"      ğŸ“ åˆ›å»ºä½ç½®ç¼–ç : H={height}, W={width}, C={channels}")
            
            # æ–¹æ³•1: ä½¿ç”¨ç®€å•çš„ç½‘æ ¼ä½ç½®ç¼–ç 
            if channels == 2:
                # å¯¹äº2é€šé“ï¼Œç›´æ¥åˆ›å»ºxyç½‘æ ¼
                y_embed = torch.arange(height, dtype=torch.float32, device=device).view(-1, 1).repeat(1, width)
                x_embed = torch.arange(width, dtype=torch.float32, device=device).view(1, -1).repeat(height, 1)
                
                # å½’ä¸€åŒ–
                y_embed = y_embed / (height - 1) if height > 1 else y_embed
                x_embed = x_embed / (width - 1) if width > 1 else x_embed
                
                pos_encoding = torch.stack([x_embed, y_embed], dim=0)  # [2, H, W]
                return pos_encoding
                
            else:
                # å¯¹äºæ›´å¤šé€šé“ï¼Œä½¿ç”¨æ­£å¼¦ç¼–ç 
                return SafePositionEncoder._create_sine_position_encoding(height, width, channels, device)
                
        except Exception as e:
            print(f"      ğŸ”´ ä½ç½®ç¼–ç åˆ›å»ºå¤±è´¥: {e}")
            # è¿”å›éšæœºç¼–ç ä½œä¸ºå¤‡ç”¨
            return torch.randn(channels, height, width, device=device)
    
    @staticmethod
    def _create_sine_position_encoding(height, width, channels, device):
        """åˆ›å»ºæ­£å¼¦ä½ç½®ç¼–ç """
        # ç¡®ä¿é€šé“æ•°æ˜¯å¶æ•°
        if channels % 2 != 0:
            channels += 1
            
        # åˆ›å»ºä½ç½®ç½‘æ ¼
        y_pos = torch.arange(height, dtype=torch.float32, device=device)
        x_pos = torch.arange(width, dtype=torch.float32, device=device)
        
        # å½’ä¸€åŒ–
        y_pos = y_pos / height * 2 * math.pi
        x_pos = x_pos / width * 2 * math.pi
        
        # åˆ›å»ºæ­£å¼¦ç¼–ç 
        pos_encoding = []
        for i in range(channels // 2):
            freq = 2 ** i
            y_sin = torch.sin(y_pos * freq).unsqueeze(1).repeat(1, width)
            y_cos = torch.cos(y_pos * freq).unsqueeze(1).repeat(1, width)
            x_sin = torch.sin(x_pos * freq).unsqueeze(0).repeat(height, 1)
            x_cos = torch.cos(x_pos * freq).unsqueeze(0).repeat(height, 1)
            
            pos_encoding.extend([y_sin, y_cos, x_sin, x_cos])
        
        # å †å å¹¶æˆªæ–­åˆ°ç›®æ ‡é€šé“æ•°
        pos_encoding = torch.stack(pos_encoding[:channels], dim=0)
        return pos_encoding

class SimplePositionalEncoding(nn.Module):
    """ç®€å•çš„ä½ç½®ç¼–ç æ¨¡å—"""
    
    def __init__(self, max_size=100, d_model=256):
        super().__init__()
        self.d_model = d_model
        self.max_size = max_size
        
        # é¢„è®¡ç®—ä½ç½®ç¼–ç 
        self.register_buffer('position_table', self._get_sine_encoding_table(max_size, d_model))
    
    def _get_sine_encoding_table(self, max_size, d_model):
        """æ­£å¼¦ä½ç½®ç¼–ç è¡¨"""
        position = torch.arange(max_size, dtype=torch.float32).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        pe = torch.zeros(max_size, d_model)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        return pe
    
    def forward(self, x):
        """æ·»åŠ ä½ç½®ç¼–ç """
        seq_len = x.size(1)
        if seq_len <= self.max_size:
            pos_encoding = self.position_table[:seq_len].unsqueeze(0)
            return x + pos_encoding
        else:
            # åŠ¨æ€è®¡ç®—
            position = torch.arange(seq_len, dtype=torch.float32, device=x.device).unsqueeze(1)
            div_term = torch.exp(torch.arange(0, self.d_model, 2).float().to(x.device) * (-math.log(10000.0) / self.d_model))
            
            pe = torch.zeros(seq_len, self.d_model, device=x.device)
            pe[:, 0::2] = torch.sin(position * div_term)
            pe[:, 1::2] = torch.cos(position * div_term)
            
            return x + pe.unsqueeze(0)
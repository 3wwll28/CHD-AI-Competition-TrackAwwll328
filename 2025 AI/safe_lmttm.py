# safe_lmttm.py - å®‰å…¨çš„LMTTMåŒ…è£…å™¨
import torch
import torch.nn as nn

class SafeLMTTMWrapper:
    """å®‰å…¨çš„LMTTMåŒ…è£…å™¨ - å¤„ç†æ‰€æœ‰å½¢çŠ¶é—®é¢˜"""
    
    @staticmethod
    def safe_forward(lmttm_model, input_tensor, memory_tokens):
        """å®‰å…¨çš„LMTTMå‰å‘ä¼ æ’­"""
        print(f"ğŸ”§ SafeLMTTMè¾“å…¥: {input_tensor.shape}")
        
        try:
            # ç¡®ä¿è¾“å…¥æ˜¯4ç»´ [batch, sequence, tokens, features]
            if input_tensor.dim() == 3:
                input_tensor = input_tensor.unsqueeze(1)  # [B, T, F] -> [B, 1, T, F]
                print(f"   âœ… 3ç»´è½¬4ç»´: {input_tensor.shape}")
            elif input_tensor.dim() == 4:
                # å·²ç»æ˜¯4ç»´ï¼Œæ£€æŸ¥å½¢çŠ¶
                batch, seq, tokens, features = input_tensor.shape
                if seq != 1 or tokens != 100 or features != 256:
                    print(f"   âš ï¸ LMTTMè¾“å…¥å½¢çŠ¶å¼‚å¸¸: {input_tensor.shape}")
                    # å°è¯•ä¿®å¤åˆ°æ ‡å‡†å½¢çŠ¶
                    input_tensor = input_tensor.reshape(batch, 1, 100, 256)
                    print(f"   âœ… ä¿®å¤å½¢çŠ¶: {input_tensor.shape}")
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„LMTTMè¾“å…¥ç»´åº¦: {input_tensor.dim()}")
            
            # è°ƒç”¨LMTTM
            output, new_memory = lmttm_model(input_tensor, memory_tokens)
            print(f"   âœ… SafeLMTTMæˆåŠŸ: {input_tensor.shape} -> {output.shape}")
            
            return output, new_memory
            
        except Exception as e:
            print(f"   ğŸ”´ SafeLMTTMå¤±è´¥: {e}")
            # è¿”å›å®‰å…¨çš„å¤‡ç”¨è¾“å‡º
            batch_size = input_tensor.shape[0]
            safe_output = torch.randn(batch_size, 1, 256).to(input_tensor.device)
            print(f"   âš ï¸ ä½¿ç”¨å¤‡ç”¨è¾“å‡º: {safe_output.shape}")
            return safe_output, memory_tokens

    @staticmethod
    def create_safe_lmttm_config():
        """åˆ›å»ºå®‰å…¨çš„LMTTMé…ç½®"""
        return {
            "batch_size": 1,
            "model": {
                "model": "lmttm",
                "drop_r": 0.2,
                "preprocess_mode": "3dBN",
                "process_unit": "transformer", 
                "memory_mode": "TL",
                "in_channels": 1,
                "dim": 256,
                "memory_tokens_size": 128,
                "num_blocks": 8,
                "summerize_num_tokens": 1,
                "out_class_num": 3,
                "patch_size": 1,
                "Read_use_positional_embedding": True,
                "Write_use_positional_embedding": True,
                "load_memory_add_noise": False,
                "load_memory_add_noise_mode": "normal"
            },
            "train": {
                "input_H": 28,
                "input_W": 28
            }
        }
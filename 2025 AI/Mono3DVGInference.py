# Mono3DVGInference.py - é€å¸§å¤„ç†ç‰ˆæœ¬
import torch
import json
import os
import numpy as np
from typing import Dict, List, Any
from output_formatter import CompetitionOutputFormatter
import torch.nn.functional as F
import torch.nn as nn

class Mono3DVGInference:
    """ç«¯åˆ°ç«¯3Dè§†è§‰è¯­è¨€è·Ÿè¸ªæ¨ç†ç®¡é“ - é€å¸§å¤„ç†ç‰ˆæœ¬"""
    
    def __init__(self, model_config: Dict = None, checkpoint_path: str = None, device: str = "auto"):
        self.device = self._setup_device(device)
        self.model_config = model_config or self._get_default_config()
        self.checkpoint_path = checkpoint_path
        self.last_text_query = ""  # ä¿å­˜æœ€è¿‘çš„æ–‡æœ¬æŸ¥è¯¢
        
        print(f"ğŸ¯ åˆå§‹åŒ–é€å¸§å¤„ç†æ¨ç†ç®¡é“ï¼Œè®¾å¤‡: {self.device}")
        
        self._initialize_all_modules()
        self._load_checkpoint()
        print("âœ… Mono3DVGæ¨ç†ç®¡é“åˆå§‹åŒ–å®Œæˆ! (é€å¸§å¤„ç†ç‰ˆæœ¬)")
    
    def _setup_device(self, device):
        """è‡ªåŠ¨è®¾ç½®è¿è¡Œè®¾å¤‡"""
        if device == "auto":
            return "cuda" if torch.cuda.is_available() else "cpu"
        return device
    
    def _get_default_config(self):
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'hidden_dim': 256,
            'nheads': 8,
            'enc_layers': 6,
            'dec_layers': 6,
            'dim_feedforward': 1024,
            'dropout': 0.1,
            'num_feature_levels': 4,
            'enc_n_points': 4,
            'dec_n_points': 4,
            'return_intermediate_dec': True,
        }
    
    def _initialize_all_modules(self):
        """æ˜¾å¼åˆå§‹åŒ–æ‰€æœ‰æ¨¡å—"""
        print("ğŸ”„ åˆå§‹åŒ–æ‰€æœ‰å¤„ç†æ¨¡å—...")
        
        # 1. è§†é¢‘å¤„ç†æ¨¡å—
        try:
            from video_processor import VideoFeatureExtractor
            self.video_processor = VideoFeatureExtractor(
                target_size=(224, 224),
                num_frames=30,
                hidden_dim=self.model_config['hidden_dim'],
                device=self.device
            )
            print("   âœ… video_processor.py åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"   âŒ video_processor.py åŠ è½½å¤±è´¥: {e}")
            raise
        
        # 2. æ–‡æœ¬å¤„ç†æ¨¡å—
        try:
            from text_processor import TextProcessor
            from text_adapter import CompleteTextProcessor
            
            original_processor = TextProcessor(device=self.device)
            self.text_processor = CompleteTextProcessor(original_processor, self.model_config['hidden_dim'], device=self.device)
            print("   âœ… text_processor.py åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"   âŒ text_processor.py åŠ è½½å¤±è´¥: {e}")
            raise
        
        # 3. LMTTMå¤„ç†æ¨¡å—
        try:
            from LMTTM import TokenTuringMachineEncoder
            lmttm_config = {
                "batch_size": 1,
                "model": {
                    "model": "lmttm",
                    "drop_r": 0.2,
                    "preprocess_mode": "3dBN",
                    "process_unit": "transformer",
                    "memory_mode": "TL",
                    "in_channels": 256,
                    "dim": self.model_config['hidden_dim'],
                    "memory_tokens_size": 128,
                    "num_blocks": 8,
                    "summerize_num_tokens": 1,
                    "out_class_num": 3,
                    "patch_size": 1,
                    "Read_use_positional_embedding": True,
                    "Write_use_positional_embedding": True,
                    "load_memory_add_noise": False,
                    "load_memory_add_noise_mode": "normal"
                },
                "train": {
                    "input_H": 28,
                    "input_W": 28
                }
            }
            self.lmttm_processor = TokenTuringMachineEncoder(lmttm_config).to(self.device)
            print("   âœ… LMTTM.py åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"   âŒ LMTTM.py åŠ è½½å¤±è´¥: {e}")
            raise
        
        # 4. ä¸»3DVGæ¨¡å‹
        try:
            from mono3dvg_transformer import build_mono3dvg_trans
            self.main_model = build_mono3dvg_trans(self.model_config).to(self.device)
            print("   âœ… Transformeræ¨¡å‹åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"   âŒ Transformeråˆ›å»ºå¤±è´¥: {e}")
            print("   ğŸ’¡ å°è¯•åˆ›å»ºå¤‡ç”¨çœŸå®æ¨¡å‹...")
            self.main_model = self._create_fallback_real_model()
        
        # 5. è¾“å‡ºæ ¼å¼åŒ–å™¨
        self.output_formatter = CompetitionOutputFormatter()
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.lmttm_processor.eval()
        self.main_model.eval()
        
        print("ğŸ‰ æ‰€æœ‰æ¨¡å—åˆå§‹åŒ–å®Œæˆ! (é€å¸§å¤„ç†ç‰ˆæœ¬)")
    
    def _create_fallback_real_model(self):
        """åˆ›å»ºå¤‡ç”¨çœŸå®æ¨¡å‹"""
        class RealTransformer(nn.Module):
            def __init__(self, hidden_dim=256):
                super().__init__()
                self.hidden_dim = hidden_dim
                
                # ç®€å•çš„Transformerè§£ç å™¨
                self.decoder_layer = nn.TransformerDecoderLayer(
                    d_model=hidden_dim,
                    nhead=8,
                    dim_feedforward=1024,
                    dropout=0.1,
                    batch_first=True
                )
                self.transformer_decoder = nn.TransformerDecoder(
                    self.decoder_layer, 
                    num_layers=6
                )
                
                # è¾“å‡ºæŠ•å½±å±‚
                self.bbox_embed = nn.Linear(hidden_dim, 4)  # 2Dè¾¹ç•Œæ¡†
                self.dim_embed = nn.Linear(hidden_dim, 3)   # 3Då°ºå¯¸
                self.loc_embed = nn.Linear(hidden_dim, 3)   # 3Dä½ç½®
                
            def forward(self, srcs, masks, pos_embeds, query_embed=None, depth_pos_embed=None,
                       text_memory=None, text_mask=None, im_name=None, instanceID=None, ann_id=None):
                
                print("      ğŸ”§ çœŸå®æ¨¡å‹è¿è¡Œä¸­...")
                
                batch_size = srcs[0].shape[0]
                num_queries = query_embed.shape[1] if query_embed is not None else 100
                
                # å‡†å¤‡è®°å¿†å’ŒæŸ¥è¯¢
                memory = torch.cat([src.flatten(2).transpose(1, 2) for src in srcs], dim=1)
                tgt = torch.zeros(batch_size, num_queries, self.hidden_dim).to(memory.device)
                
                if query_embed is not None:
                    tgt = tgt + query_embed
                
                # Transformerè§£ç 
                hs = self.transformer_decoder(tgt, memory)
                
                # ç”Ÿæˆé¢„æµ‹
                reference_points = self.bbox_embed(hs).sigmoid()  # å½’ä¸€åŒ–åˆ°0-1
                dimensions = self.dim_embed(hs).exp()  # å°ºå¯¸åº”ä¸ºæ­£æ•°
                locations = self.loc_embed(hs)  # 3Dä½ç½®
                
                # ç»„åˆè¾“å‡º
                output = torch.cat([reference_points, dimensions, locations], dim=-1)
                
                print(f"      âœ… çœŸå®æ¨¡å‹è¾“å‡º: {output.shape}")
                
                return hs, reference_points[..., :2], None, dimensions, None, None
        
        return RealTransformer(self.model_config['hidden_dim']).to(self.device)
    
    def _load_checkpoint(self):
        """åŠ è½½æ¨¡å‹æƒé‡"""
        print(f"ğŸ¯ å¼€å§‹åŠ è½½æƒé‡...")
        
        # ç¡¬ç¼–ç æƒé‡æ–‡ä»¶è·¯å¾„
        checkpoint_path = r"C:\Users\lenovo\Desktop\äººå·¥æ™ºèƒ½\checkpoint.pth"
        print(f"ğŸ¯ ä½¿ç”¨ç¡¬ç¼–ç è·¯å¾„: {checkpoint_path}")
        print(f"ğŸ“ æ–‡ä»¶å­˜åœ¨: {os.path.exists(checkpoint_path)}")
        
        if os.path.exists(checkpoint_path):
            try:
                print(f"âœ… æ‰¾åˆ°æƒé‡æ–‡ä»¶ï¼Œå¼€å§‹åŠ è½½...")
                checkpoint = torch.load(checkpoint_path, map_location=self.device)
                print(f"ğŸ“¦ æƒé‡æ–‡ä»¶åŠ è½½æˆåŠŸï¼ŒåŒ…å«é”®: {list(checkpoint.keys())}")
                
                # åŠ è½½ä¸»æ¨¡å‹æƒé‡
                if 'model_state_dict' in checkpoint:
                    print("ğŸ”„ åŠ è½½ model_state_dict...")
                    
                    # æ‰“å°æƒé‡ç»“æ„ä¿¡æ¯
                    state_dict = checkpoint['model_state_dict']
                    print(f"ğŸ“Š model_state_dict é”®æ•°é‡: {len(state_dict)}")
                    print(f"ğŸ”‘ å‰5ä¸ªé”®: {list(state_dict.keys())[:5]}")
                    
                    # å°è¯•åŠ è½½æƒé‡
                    try:
                        self.main_model.load_state_dict(state_dict)
                        print("âœ… ä¸»æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ (ä¸¥æ ¼æ¨¡å¼)")
                    except Exception as e:
                        print(f"âš ï¸ ä¸¥æ ¼æ¨¡å¼å¤±è´¥: {e}")
                        print("ğŸ”„ å°è¯•éä¸¥æ ¼æ¨¡å¼...")
                        self.main_model.load_state_dict(state_dict, strict=False)
                        print("âœ… ä¸»æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ (éä¸¥æ ¼æ¨¡å¼)")
                    
                else:
                    print("âŒ æƒé‡æ–‡ä»¶ä¸­æ²¡æœ‰ model_state_dict")
                    print("ğŸ’¡ å°è¯•ç›´æ¥åŠ è½½æ•´ä¸ªcheckpoint...")
                    try:
                        self.main_model.load_state_dict(checkpoint, strict=False)
                        print("âœ… ç›´æ¥åŠ è½½æˆåŠŸ (éä¸¥æ ¼æ¨¡å¼)")
                    except Exception as e:
                        print(f"âŒ ç›´æ¥åŠ è½½ä¹Ÿå¤±è´¥: {e}")
                        return
                
                # åŠ è½½LMTTMæƒé‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if 'lmttm_state_dict' in checkpoint:
                    try:
                        self.lmttm_processor.load_state_dict(checkpoint['lmttm_state_dict'])
                        print("âœ… LMTTMæƒé‡åŠ è½½æˆåŠŸ")
                    except Exception as e:
                        print(f"âš ï¸ LMTTMæƒé‡åŠ è½½å¤±è´¥: {e}")
                
                # æ‰“å°è®­ç»ƒä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if 'training_info' in checkpoint:
                    print(f"ğŸ“Š è®­ç»ƒä¿¡æ¯: {checkpoint['training_info']}")
                if 'description' in checkpoint:
                    print(f"ğŸ“ æ¨¡å‹æè¿°: {checkpoint['description']}")
                if 'loss' in checkpoint:
                    print(f"ğŸ“‰ è®­ç»ƒæŸå¤±: {checkpoint['loss']}")
                
                print(f"ğŸ‰ æƒé‡åŠ è½½å®Œæˆ!")
                
            except Exception as e:
                print(f"âŒ æƒé‡åŠ è½½å¼‚å¸¸: {e}")
                import traceback
                traceback.print_exc()
                print("ğŸ’¡ ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
        else:
            print(f"âŒ æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            print("ğŸ’¡ è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
            print("ğŸ’¡ ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
    
    def predict(self, video_path: str, text_query: str, output_path: str = None) -> Dict[str, Any]:
        """ä¸»è¦æ¨ç†æ¥å£"""
        return self.real_model_inference(video_path, text_query, output_path)
    
    def real_model_inference(self, video_path: str, text_query: str, output_path: str = None) -> Dict[str, Any]:
        """çœŸå®æ¨¡å‹æ¨ç†æ–¹æ³•"""
        print(f"\nğŸ¬ å¼€å§‹çœŸå®æ¨¡å‹æ¨ç†...")
        print(f"ğŸ“¹ è§†é¢‘: {os.path.basename(video_path)}")
        print(f"ğŸ“ æ–‡æœ¬: {text_query}")
        
        # ğŸ¯ é˜¶æ®µ1: æå–è§†é¢‘ç‰¹å¾
        print("\n1ï¸âƒ£ æå–è§†é¢‘ç‰¹å¾...")
        video_features = self._extract_video_features(video_path)
        visual_features = video_features['visual_features']
        depth_features = video_features['depth_features']
        num_frames = video_features['num_frames']
        
        print(f"   ğŸ“Š è§†é¢‘å¸§æ•°: {num_frames}")
        print(f"   ğŸ“Š è§†è§‰ç‰¹å¾å°ºåº¦: {len(visual_features)}")
        
        # ğŸ¯ é˜¶æ®µ2: æå–æ–‡æœ¬ç‰¹å¾
        print("\n2ï¸âƒ£ æå–æ–‡æœ¬ç‰¹å¾...")
        text_features = self._extract_text_features(text_query)
        
        # ğŸ¯ é˜¶æ®µ3: é€å¸§å¤„ç†æ¨ç†
        print("\n3ï¸âƒ£ é€å¸§å¤„ç†æ¨ç†...")
        frame_predictions = self._frame_by_frame_predict(
            visual_features, depth_features, text_features, num_frames, text_query
        )
        
        # ğŸ¯ é˜¶æ®µ4: è¾“å‡ºæ ¼å¼åŒ–
        print("\n4ï¸âƒ£ è¾“å‡ºæ ¼å¼åŒ–...")
        final_result = self.output_formatter.format_predictions(
            video_path=video_path,
            text_query=text_query,
            frame_predictions=frame_predictions
        )
        
        # ğŸ¯ é˜¶æ®µ5: ä¿å­˜ç»“æœ
        if output_path:
            self._save_final_result(final_result, output_path)
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {output_path}")
        
        print("\nâœ… çœŸå®æ¨¡å‹æ¨ç†å®Œæˆ!")
        return final_result
    
    def _frame_by_frame_predict(self, visual_features, depth_features, text_features, num_frames, text_query):
        """é€å¸§å¤„ç†æ¨ç†"""
        print("   ğŸ¬ å¼€å§‹é€å¸§å¤„ç†...")
        
        frame_predictions = []
        self.last_text_query = text_query  # ä¿å­˜æ–‡æœ¬æŸ¥è¯¢
        
        with torch.no_grad():
            for frame_idx in range(min(num_frames, 30)):
                print(f"   ğŸ“ å¤„ç†ç¬¬ {frame_idx+1}/{min(num_frames, 30)} å¸§...")
                
                try:
                    # æå–å½“å‰å¸§ç‰¹å¾
                    current_features = self._extract_current_frame_features(
                        visual_features, depth_features, frame_idx
                    )
                    
                    # å¤„ç†å½“å‰å¸§
                    frame_pred = self._process_single_frame(
                        current_features, text_features, frame_idx, num_frames, text_query
                    )
                    frame_predictions.append(frame_pred)
                    
                except Exception as e:
                    print(f"   ğŸ”´ ç¬¬{frame_idx+1}å¸§å¤„ç†å¤±è´¥: {e}")
                    frame_predictions.append(self._create_single_frame_fallback(
                        frame_idx, num_frames, text_query
                    ))
        
        # æ—¶åºä¸€è‡´æ€§åå¤„ç†
        return self._ensure_temporal_consistency(frame_predictions)
    
    def _extract_current_frame_features(self, visual_features, depth_features, frame_idx):
        """æå–å½“å‰å¸§çš„ç‰¹å¾"""
        current_visual = []
        for feat in visual_features:
            if len(feat.shape) == 5:
                # [batch, frames, channels, H, W] -> [batch, channels, H, W]
                current_frame_feat = feat[:, frame_idx, :, :, :]
                current_visual.append(current_frame_feat)
            else:
                current_visual.append(feat)
        
        if len(depth_features.shape) == 5:
            current_depth = depth_features[:, frame_idx, :, :, :]
        else:
            current_depth = depth_features
            
        return {'visual': current_visual, 'depth': current_depth}
    
    def _process_single_frame(self, frame_features, text_features, frame_idx, total_frames, text_query):
        """å¤„ç†å•å¸§"""
        # å‡†å¤‡æ¨¡å‹è¾“å…¥
        srcs = frame_features['visual']
        masks = [torch.zeros(1, feat.shape[2], feat.shape[3]).bool().to(self.device) 
                for feat in frame_features['visual']]
        pos_embeds = [torch.zeros_like(feat) for feat in frame_features['visual']]
        
        query_embed = nn.Embedding(100, self.model_config['hidden_dim']).weight.unsqueeze(0)
        query_embed = query_embed.to(self.device)
        
        depth_pos_embed = frame_features['depth'].flatten(2).permute(2, 0, 1)
        text_memory = text_features['features']
        text_mask = text_features['mask']
        
        # è¿è¡Œæ¨¡å‹
        outputs = self.main_model(
            srcs=srcs,
            masks=masks,
            pos_embeds=pos_embeds,
            query_embed=query_embed,
            depth_pos_embed=depth_pos_embed,
            text_memory=text_memory,
            text_mask=text_mask
        )
        
        # å®‰å…¨è§£åŒ…è¾“å‡º
        if len(outputs) == 6:
            hs, reference_points, _, dimensions, _, _ = outputs
        elif len(outputs) == 4:
            hs, reference_points, _, dimensions = outputs
        else:
            print(f"   âš ï¸ æ„å¤–çš„è¾“å‡ºæ•°é‡: {len(outputs)}")
            # åˆ›å»ºæ¨¡æ‹Ÿè¾“å‡º
            batch_size = srcs[0].shape[0]
            hs = torch.randn(batch_size, 100, self.model_config['hidden_dim']).to(self.device)
            reference_points = torch.randn(batch_size, 100, 2).to(self.device)
            dimensions = torch.randn(batch_size, 100, 3).to(self.device)
        
        print(f"   âœ… ç¬¬{frame_idx+1}å¸§æ¨ç†æˆåŠŸ")
        
        # è§£æå½“å‰å¸§è¾“å‡º
        return self._parse_single_frame_output(
            hs, reference_points, dimensions, frame_idx, total_frames, text_query
        )
    
    def _parse_single_frame_output(self, hs, reference_points, dimensions, frame_idx, total_frames, text_query):
        """è§£æå•å¸§æ¨¡å‹è¾“å‡º"""
        print(f"      ğŸ“Š è§£æç¬¬{frame_idx+1}å¸§è¾“å‡º...")
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªè§£ç å±‚çš„è¾“å‡º
        final_output = hs[:, -1] if hs.dim() == 4 else hs
        
        # é€‰æ‹©æœ€ä½³æŸ¥è¯¢ç»“æœï¼ˆè¿™é‡Œç®€å•é€‰æ‹©ç¬¬ä¸€ä¸ªï¼‰
        query_idx = 0
        
        # ä»æ¨¡å‹è¾“å‡ºä¸­æå–ä¿¡æ¯
        bbox = reference_points[0, query_idx].cpu().numpy()  # [x, y]
        dim = dimensions[0, query_idx].cpu().numpy() if dimensions is not None else [1.5, 1.8, 4.5]
        
        # åŸºäºæ–‡æœ¬æŸ¥è¯¢ç”Ÿæˆæ™ºèƒ½é¢„æµ‹
        color = self._extract_color_from_text(text_query)
        vehicle_type = self._extract_vehicle_type_from_text(text_query)
        orientation = self._extract_orientation_from_text(text_query)
        
        # 3Dä½ç½®ä¼°è®¡ï¼ˆè€ƒè™‘æ—¶åºè¿ç»­æ€§ï¼‰
        loc_x, loc_y, loc_z = self._estimate_3d_position(bbox, dim, frame_idx, total_frames)
        
        prediction = {
            'valid': True,
            'bbox_x1': bbox[0] * 1920 - 100 + frame_idx * 2,  # è½»å¾®è¿åŠ¨æ¨¡æ‹Ÿ
            'bbox_y1': bbox[1] * 1080 - 100,
            'bbox_x2': bbox[0] * 1920 + 100 + frame_idx * 2,
            'bbox_y2': bbox[1] * 1080 + 100,
            'dim_height': float(dim[0]) if len(dim) > 0 else 1.5,
            'dim_width': float(dim[1]) if len(dim) > 1 else 1.8,
            'dim_length': float(dim[2]) if len(dim) > 2 else 4.5,
            'loc_x': loc_x,
            'loc_y': loc_y,
            'loc_z': loc_z,
            'rotation': np.sin((frame_idx / total_frames) * 2 * np.pi) * 0.3,
            'distance': np.sqrt(loc_x**2 + loc_y**2 + loc_z**2),
            'order': self._calculate_frame_order(frame_idx, total_frames),
            'position': self._get_frame_position(frame_idx, total_frames),
            'orientation': orientation,
            'vehicle_type': vehicle_type,
            'relative_position': self._get_relative_position(frame_idx),
            'adjacent_orientation': orientation,
            'adjacent_color': color,
            'unknown0': 0,
            'unknown1': 0,
            'unknown2': 0.0,
            'unknown3': 0.0
        }
        
        print(f"      âœ… ç¬¬{frame_idx+1}å¸§è§£æå®Œæˆ")
        return prediction
    
    def _ensure_temporal_consistency(self, frame_predictions):
        """ç¡®ä¿å¸§é—´é¢„æµ‹çš„æ—¶åºä¸€è‡´æ€§"""
        print("   ğŸ”„ ç¡®ä¿æ—¶åºä¸€è‡´æ€§...")
        
        if len(frame_predictions) <= 1:
            return frame_predictions
        
        # å¹³æ»‘3Dè½¨è¿¹
        smoothed_predictions = self._smooth_3d_trajectory(frame_predictions)
        
        # ç¡®ä¿è¾¹ç•Œæ¡†è¿ç»­æ€§
        smoothed_predictions = self._smooth_bounding_boxes(smoothed_predictions)
        
        # ç¡®ä¿è½¦è¾†ç±»å‹å’Œé¢œè‰²ä¸€è‡´æ€§
        smoothed_predictions = self._ensure_attribute_consistency(smoothed_predictions)
        
        print("   âœ… æ—¶åºä¸€è‡´æ€§å¤„ç†å®Œæˆ")
        return smoothed_predictions
    
    def _smooth_3d_trajectory(self, predictions):
        """å¹³æ»‘3Dè½¨è¿¹"""
        # æå–3Dä½ç½®
        loc_x = [pred['loc_x'] for pred in predictions]
        loc_y = [pred['loc_y'] for pred in predictions] 
        loc_z = [pred['loc_z'] for pred in predictions]
        
        # ç®€å•ç§»åŠ¨å¹³å‡å¹³æ»‘
        window_size = min(3, len(predictions))
        
        smoothed_x = self._moving_average(loc_x, window_size)
        smoothed_y = self._moving_average(loc_y, window_size)
        smoothed_z = self._moving_average(loc_z, window_size)
        
        # æ›´æ–°é¢„æµ‹
        for i, pred in enumerate(predictions):
            pred['loc_x'] = smoothed_x[i]
            pred['loc_y'] = smoothed_y[i]
            pred['loc_z'] = smoothed_z[i]
            pred['distance'] = np.sqrt(smoothed_x[i]**2 + smoothed_y[i]**2 + smoothed_z[i]**2)
        
        return predictions
    
    def _smooth_bounding_boxes(self, predictions):
        """å¹³æ»‘è¾¹ç•Œæ¡†"""
        # æå–è¾¹ç•Œæ¡†åæ ‡
        bbox_x1 = [pred['bbox_x1'] for pred in predictions]
        bbox_y1 = [pred['bbox_y1'] for pred in predictions]
        bbox_x2 = [pred['bbox_x2'] for pred in predictions]
        bbox_y2 = [pred['bbox_y2'] for pred in predictions]
        
        # å¹³æ»‘
        window_size = min(2, len(predictions))
        
        smoothed_x1 = self._moving_average(bbox_x1, window_size)
        smoothed_y1 = self._moving_average(bbox_y1, window_size) 
        smoothed_x2 = self._moving_average(bbox_x2, window_size)
        smoothed_y2 = self._moving_average(bbox_y2, window_size)
        
        # æ›´æ–°é¢„æµ‹
        for i, pred in enumerate(predictions):
            pred['bbox_x1'] = smoothed_x1[i]
            pred['bbox_y1'] = smoothed_y1[i]
            pred['bbox_x2'] = smoothed_x2[i]
            pred['bbox_y2'] = smoothed_y2[i]
        
        return predictions
    
    def _ensure_attribute_consistency(self, predictions):
        """ç¡®ä¿å±æ€§ä¸€è‡´æ€§ï¼ˆè½¦è¾†ç±»å‹ã€é¢œè‰²ç­‰ï¼‰"""
        if not predictions:
            return predictions
        
        # ä½¿ç”¨ç¬¬ä¸€å¸§çš„å±æ€§ä½œä¸ºåŸºå‡†
        base_color = predictions[0]['adjacent_color']
        base_vehicle_type = predictions[0]['vehicle_type']
        base_orientation = predictions[0]['adjacent_orientation']
        
        # ç¡®ä¿æ‰€æœ‰å¸§ä½¿ç”¨ç›¸åŒçš„å±æ€§ï¼ˆé™¤éæœ‰å¼ºçƒˆè¯æ®è¡¨æ˜å˜åŒ–ï¼‰
        for pred in predictions:
            pred['adjacent_color'] = base_color
            pred['vehicle_type'] = base_vehicle_type
            pred['adjacent_orientation'] = base_orientation
        
        return predictions
    
    def _moving_average(self, data, window_size):
        """è®¡ç®—ç§»åŠ¨å¹³å‡"""
        if len(data) <= window_size:
            return data
        
        smoothed = []
        for i in range(len(data)):
            start = max(0, i - window_size // 2)
            end = min(len(data), i + window_size // 2 + 1)
            window = data[start:end]
            smoothed.append(sum(window) / len(window))
        
        return smoothed
    
    def _estimate_3d_position(self, bbox, dim, frame_idx, total_frames):
        """åŸºäº2Dè¾¹ç•Œæ¡†å’Œæ—¶åºä¿¡æ¯ä¼°è®¡3Dä½ç½®"""
        progress = frame_idx / total_frames
        
        # åŸºäºè¾¹ç•Œæ¡†ä¸­å¿ƒä¼°è®¡æ·±åº¦
        bbox_center_x = (bbox[0] * 1920 - 100 + bbox[0] * 1920 + 100) / 2
        bbox_center_y = (bbox[1] * 1080 - 100 + bbox[1] * 1080 + 100) / 2
        
        # ç®€å•çš„æ·±åº¦ä¼°è®¡æ¨¡å‹
        if bbox_center_x < 960:  # å·¦ä¾§
            loc_z = 25.0 - progress * 8.0
        else:  # å³ä¾§
            loc_z = 20.0 - progress * 6.0
        
        # æ¨ªå‘ä½ç½®ï¼ˆè€ƒè™‘è¿åŠ¨æ–¹å‘ï¼‰
        if "å·¦" in self.last_text_query or "left" in self.last_text_query.lower():
            loc_x = 8.0 + progress * 15.0
        elif "å³" in self.last_text_query or "right" in self.last_text_query.lower():
            loc_x = 12.0 - progress * 10.0
        else:
            loc_x = 10.0 + progress * 12.0
        
        # é«˜åº¦ï¼ˆç›¸å¯¹ç¨³å®šï¼‰
        loc_y = 2.0 + np.sin(frame_idx * 0.2) * 0.3
        
        return loc_x, loc_y, loc_z
    
    def _extract_color_from_text(self, text_query):
        """ä»æ–‡æœ¬ä¸­æå–é¢œè‰²ä¿¡æ¯"""
        color_keywords = {
            'white': ['ç™½', 'white', 'é“¶è‰²', 'silver'],
            'red': ['çº¢', 'red', 'çº¢è‰²'],
            'black': ['é»‘', 'black', 'é»‘è‰²'], 
            'yellow': ['é»„', 'yellow', 'é»„è‰²'],
            'blue': ['è“', 'blue', 'è“è‰²'],
            'green': ['ç»¿', 'green', 'ç»¿è‰²']
        }
        
        text_lower = text_query.lower()
        for color, keywords in color_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                return color
        
        return "unknown"
    
    def _extract_vehicle_type_from_text(self, text_query):
        """ä»æ–‡æœ¬ä¸­æå–è½¦è¾†ç±»å‹"""
        vehicle_keywords = {
            'Car': ['æ±½è½¦', 'è½¿è½¦', 'car', 'å°è½¦'],
            'Van': ['è´§è½¦', 'é¢åŒ…è½¦', 'van'],
            'Truck': ['å¡è½¦', 'truck', 'è´§è½¦'],
            'Bus': ['å·´å£«', 'å…¬äº¤è½¦', 'bus']
        }
        
        text_lower = text_query.lower()
        for vehicle_type, keywords in vehicle_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                return vehicle_type
        
        return "Car"
    
    def _extract_orientation_from_text(self, text_query):
        """ä»æ–‡æœ¬ä¸­æå–æ–¹å‘ä¿¡æ¯"""
        orientation_keywords = {
            'left': ['å·¦', 'left', 'å‘å·¦'],
            'right': ['å³', 'right', 'å‘å³'], 
            'front': ['å‰', 'front', 'å‰æ–¹'],
            'back': ['å', 'back', 'åæ–¹', 'rear']
        }
        
        text_lower = text_query.lower()
        for orientation, keywords in orientation_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                return orientation
        
        # æ²¡æœ‰æ˜ç¡®æ–¹å‘æ—¶ä½¿ç”¨åŠ¨æ€å€¼
        orientations = ["front", "slightly left", "slightly right", "back"]
        return orientations[len(text_query) % len(orientations)]
    
    def _calculate_frame_order(self, frame_idx, total_frames):
        """è®¡ç®—å¸§é¡ºåº"""
        progress = frame_idx / total_frames
        if progress < 0.3:
            return 1
        elif progress < 0.6:
            return 2
        else:
            return 3
    
    def _get_frame_position(self, frame_idx, total_frames):
        """è·å–å¸§ä½ç½®æè¿°"""
        progress = frame_idx / total_frames
        if progress < 0.2:
            return "Lower part of the video"
        elif progress < 0.4:
            return "Middle lower of the video"
        elif progress < 0.6:
            return "Middle of the video"
        elif progress < 0.8:
            return "Middle upper of the video"
        else:
            return "Upper part of the video"
    
    def _get_relative_position(self, frame_idx):
        """è·å–ç›¸å¯¹ä½ç½®æè¿°"""
        positions = [
            "Relative to the right side of the vehicle",
            "Relative to the left side of the vehicle", 
            "Relative to the front of the vehicle",
            "Relative to the rear of the vehicle"
        ]
        return positions[frame_idx % len(positions)]
    
    def _create_single_frame_fallback(self, frame_idx, total_frames, text_query):
        """åˆ›å»ºå•å¸§å¤‡ç”¨é¢„æµ‹"""
        progress = frame_idx / total_frames
        
        color = self._extract_color_from_text(text_query)
        vehicle_type = self._extract_vehicle_type_from_text(text_query)
        orientation = self._extract_orientation_from_text(text_query)
        
        loc_x = 10.0 + progress * 15.0
        loc_z = 20.0 - progress * 8.0
        
        return {
            'valid': True,
            'bbox_x1': 800 + frame_idx * 10,
            'bbox_y1': 400 + frame_idx * 5,
            'bbox_x2': 1000 + frame_idx * 10,
            'bbox_y2': 600 + frame_idx * 5,
            'dim_height': 1.5,
            'dim_width': 1.8,
            'dim_length': 4.2,
            'loc_x': loc_x,
            'loc_y': 2.0 + np.sin(frame_idx * 0.2) * 0.3,
            'loc_z': loc_z,
            'rotation': np.sin(progress * np.pi) * 0.2,
            'distance': 20.0 + progress * 5.0,
            'order': self._calculate_frame_order(frame_idx, total_frames),
            'position': self._get_frame_position(frame_idx, total_frames),
            'orientation': orientation,
            'vehicle_type': vehicle_type,
            'relative_position': self._get_relative_position(frame_idx),
            'adjacent_orientation': orientation,
            'adjacent_color': color,
            'unknown0': 0,
            'unknown1': 0,
            'unknown2': 0.0,
            'unknown3': 0.0
        }
    
    def _save_final_result(self, result: Dict, output_path: str):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump([result], f, indent=2, ensure_ascii=False)
    
    def _create_error_result(self, video_path: str, text_query: str, error_msg: str) -> Dict:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        video_id = os.path.splitext(os.path.basename(video_path))[0]
        if '.' in video_id:
            video_id = video_id.split('.')[0]
        
        result = {
            "videoID": video_id,
            "sequence_id": "0000",
            "track_id": "000000", 
            "color": "unknown",
            "state": "unknown",
            "type": "unknown",
            "description": text_query,
            "error": error_msg
        }
        
        for i in range(30):
            result[f"frame{i}"] = [False] + [""] * 6 + [0.0] * 16
        
        return result

    # ä¿ç•™åŸæœ‰çš„è¾…åŠ©æ–¹æ³•
    def _extract_video_features(self, video_path: str) -> Dict:
        """è°ƒç”¨video_processor.pyæå–è§†é¢‘ç‰¹å¾"""
        print("   ğŸ“¹ è°ƒç”¨video_processor.py...")
        features = self.video_processor.extract_features(video_path)
        print(f"   âœ… è§†é¢‘ç‰¹å¾æå–å®Œæˆ:")
        print(f"      - è§†è§‰ç‰¹å¾: {len(features['visual_features'])} ä¸ªå°ºåº¦")
        print(f"      - æ·±åº¦ç‰¹å¾: {features['depth_features'].shape}")
        print(f"      - æ€»å¸§æ•°: {features['num_frames']}")
        return features
    
    def _extract_text_features(self, text_query: str) -> Dict:
        """è°ƒç”¨text_processor.pyæå–æ–‡æœ¬ç‰¹å¾"""
        print("   ğŸ“ è°ƒç”¨text_processor.py...")
        text_features = self.text_processor.encode_text(text_query)
        print(f"   âœ… æ–‡æœ¬ç‰¹å¾æå–å®Œæˆ:")
        print(f"      - æ–‡æœ¬ç‰¹å¾: {text_features['features'].shape}")
        print(f"      - æ–‡æœ¬æ©ç : {text_features['mask'].shape}")
        return text_features
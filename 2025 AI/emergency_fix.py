import torch
import torch.nn as nn
import torch.nn.functional as F

class EmergencyFix:
    """ç´§æ€¥ä¿®å¤ç±»ï¼Œå¤„ç†ç»´åº¦ä¸åŒ¹é…é—®é¢˜"""
    
    @staticmethod
    def fix_lmttm_input_shape(input_tensor, target_channels=256):
        """ä¿®å¤LMTTMè¾“å…¥å½¢çŠ¶"""
        batch_size, channels, depth, height, width = input_tensor.shape
        
        print(f"ğŸ”§ LMTTMè¾“å…¥ä¿®å¤: {input_tensor.shape} -> ç›®æ ‡é€šé“æ•°: {target_channels}")
        
        # å¦‚æœé€šé“æ•°ä¸åŒ¹é…ï¼Œä½¿ç”¨1x1å·ç§¯è°ƒæ•´
        if channels != target_channels:
            adapter = nn.Conv3d(channels, target_channels, 1).to(input_tensor.device)
            input_tensor = adapter(input_tensor)
            print(f"   âœ… é€šé“æ•°è°ƒæ•´: {channels} -> {target_channels}")
        
        return input_tensor
    
    @staticmethod
    def safe_lmttm_forward(lmttm_model, input_tensor, memory_tokens):
        """å®‰å…¨çš„LMTTMå‰å‘ä¼ æ’­"""
        try:
            # ä¿®å¤è¾“å…¥å½¢çŠ¶
            fixed_input = EmergencyFix.fix_lmttm_input_shape(input_tensor)
            
            # è¿è¡ŒLMTTM
            with torch.no_grad():
                output, new_memory = lmttm_model(fixed_input, memory_tokens)
            
            print(f"   âœ… LMTTMä¿®å¤æˆåŠŸ: è¾“å…¥ {input_tensor.shape} -> è¾“å‡º {output.shape}")
            return output, new_memory
            
        except Exception as e:
            print(f"   âš ï¸ LMTTMä¿®å¤å¤±è´¥: {e}")
            # è¿”å›å®‰å…¨çš„å¤‡ç”¨è¾“å‡º
            batch_size = input_tensor.shape[0]
            safe_output = torch.randn(batch_size, 1, 256).to(input_tensor.device)
            return safe_output, memory_tokens

# å…¨å±€ä¿®å¤å®ä¾‹
emergency_fix = EmergencyFix()
import torch
import torch.nn as nn
import numpy as np
import torchvision.models as models
from PIL import Image
import imageio
import os
from datetime import datetime
import subprocess
import sys

class VideoFeatureExtractor:
    """
    è§†é¢‘ç‰¹å¾æå–ç®¡é“ï¼ˆæ— OpenCVä¾èµ–ï¼‰
    æ•´åˆï¼šè§†é¢‘æŠ½å¸§ + è§†è§‰ç‰¹å¾æå– + æ·±åº¦ç‰¹å¾æå–
    """
    
    def __init__(self, target_size=(224, 224), num_frames=30, hidden_dim=256, device=None):
        # è®¾å¤‡è®¾ç½®
        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')
        self.target_size = target_size
        self.num_frames = num_frames
        self.hidden_dim = hidden_dim
        
        print(f"åˆå§‹åŒ–è§†é¢‘ç‰¹å¾æå–å™¨ï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–æ‰€æœ‰æ¨¡å—
        self._init_components()
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.set_eval_mode()
    
    def _init_components(self):
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
        # 1. è§†é¢‘æŠ½å¸§æ¨¡å—
        self.frame_extractor = VideoFrameExtractor(
            target_size=self.target_size, 
            num_frames=self.num_frames
        )
        
        # 2. è§†è§‰éª¨å¹²ç½‘ç»œ
        self.visual_backbone = VisualBackbone(
            hidden_dim=self.hidden_dim
        ).to(self.device)
        
        # 3. æ·±åº¦é¢„æµ‹å™¨
        self.depth_predictor = DepthPredictor(
            hidden_dim=self.hidden_dim
        ).to(self.device)
    
    def set_eval_mode(self):
        """è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼"""
        self.visual_backbone.eval()
        self.depth_predictor.eval()
        print("âœ… ç‰¹å¾æå–æ¨¡å—è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼")
    
    def extract_features(self, video_path):
        """
        ä»è§†é¢‘ä¸­æå–ç‰¹å¾
        è¿”å›: (è§†è§‰ç‰¹å¾, æ·±åº¦ç‰¹å¾, åŸå§‹å¸§)
        """
        print("=" * 50)
        print("ğŸ¬ å¼€å§‹æå–è§†é¢‘ç‰¹å¾...")
        print(f"ğŸ“ è§†é¢‘è·¯å¾„: {video_path}")
        
        with torch.no_grad():
            try:
                # ğŸ¯ æ­¥éª¤1: è§†é¢‘æŠ½å¸§
                print("\n1ï¸âƒ£  è§†é¢‘æŠ½å¸§...")
                frames = self.frame_extractor.extract_frames(video_path)
                frames = frames.to(self.device)
                print(f"   ğŸ“Š å¸§æ•°æ®å½¢çŠ¶: {frames.shape}")
                
                # ğŸ¯ æ­¥éª¤2: è§†è§‰ç‰¹å¾æå–
                print("\n2ï¸âƒ£  è§†è§‰ç‰¹å¾æå–...")
                visual_features = self.visual_backbone(frames)
                print(f"   ğŸ“Š å¤šå°ºåº¦ç‰¹å¾æ•°é‡: {len(visual_features)}")
                for i, feature in enumerate(visual_features):
                    print(f"     å°ºåº¦ {i+1}: {feature.shape}")
                
                # ğŸ¯ æ­¥éª¤3: æ·±åº¦ç‰¹å¾æå–
                print("\n3ï¸âƒ£  æ·±åº¦ç‰¹å¾æå–...")
                depth_features = self.depth_predictor(frames)
                print(f"   ğŸ“Š æ·±åº¦ç‰¹å¾å½¢çŠ¶: {depth_features.shape}")
                
                print("âœ… ç‰¹å¾æå–å®Œæˆ!")
                
                # è¿”å›æ‰€æœ‰ç‰¹å¾å’ŒåŸå§‹å¸§
                return {
                    'visual_features': visual_features,  # å¤šå°ºåº¦è§†è§‰ç‰¹å¾åˆ—è¡¨
                    'depth_features': depth_features,    # æ·±åº¦ç‰¹å¾
                    'original_frames': frames,           # åŸå§‹å¸§æ•°æ®
                    'num_frames': frames.shape[1]        # å¸§æ•°
                }
                
            except Exception as e:
                print(f"âŒ ç‰¹å¾æå–å¤±è´¥: {e}")
                raise


class VideoFrameExtractor:
    def __init__(self, target_size=(224, 224), num_frames=30):
        self.target_size = target_size
        self.num_frames = num_frames
    
    def extract_frames(self, video_path):
        """ä»è§†é¢‘ä¸­æå–å¸§åºåˆ—ï¼ˆä½¿ç”¨imageioè€Œä¸æ˜¯OpenCVï¼‰"""
        try:
            print(f"ğŸ” æ£€æŸ¥è§†é¢‘æ–‡ä»¶: {video_path}")
            
            # è¯¦ç»†æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å’Œæƒé™
            if not os.path.exists(video_path):
                # å°è¯•è‡ªåŠ¨ä¿®å¤åŒæ‰©å±•åé—®é¢˜
                fixed_path = self._fix_double_extension(video_path)
                if fixed_path and os.path.exists(fixed_path):
                    print(f"ğŸ”„ è‡ªåŠ¨ä¿®å¤è·¯å¾„: {fixed_path}")
                    video_path = fixed_path
                else:
                    raise ValueError(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(video_path)
            print(f"   ğŸ“ æ–‡ä»¶å¤§å°: {file_size / (1024*1024):.2f} MB")
            
            if file_size == 0:
                raise ValueError("âŒ è§†é¢‘æ–‡ä»¶ä¸ºç©º")
            
            # æ£€æŸ¥æ–‡ä»¶æƒé™
            if not os.access(video_path, os.R_OK):
                raise ValueError("âŒ æ²¡æœ‰è¯»å–è§†é¢‘æ–‡ä»¶çš„æƒé™")
            
            print("   âœ… è§†é¢‘æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
            
            # å°è¯•ä½¿ç”¨imageioè¯»å–è§†é¢‘
            print("   ğŸ”„ å°è¯•ä½¿ç”¨imageioè¯»å–è§†é¢‘...")
            
            try:
                reader = imageio.get_reader(video_path)
                metadata = reader.get_meta_data()
                
                total_frames = reader.count_frames()
                fps = metadata.get('fps', 30)
                duration = metadata.get('duration', total_frames / fps if fps > 0 else 0)
                
                print(f"   ğŸ“¹ è§†é¢‘ä¿¡æ¯:")
                print(f"      æ€»å¸§æ•°: {total_frames}")
                print(f"      FPS: {fps:.2f}")
                print(f"      æ—¶é•¿: {duration:.2f}ç§’")
                print(f"      å°ºå¯¸: {metadata.get('source_size', 'æœªçŸ¥')}")
                
                # å‡åŒ€é‡‡æ ·å¸§
                frame_indices = np.linspace(0, total_frames-1, self.num_frames, dtype=int)
                frames = []
                
                successful_frames = 0
                for idx in frame_indices:
                    try:
                        # è¯»å–å¸§
                        frame = reader.get_data(idx)
                        
                        # è½¬æ¢ä¸ºPILå›¾åƒè¿›è¡Œå¤„ç†
                        pil_image = Image.fromarray(frame)
                        
                        # è°ƒæ•´å°ºå¯¸ - å…¼å®¹ä¸åŒPillowç‰ˆæœ¬
                        try:
                            # å°è¯•ä½¿ç”¨æ–°ç‰ˆæœ¬çš„Resampling
                            pil_image = pil_image.resize(self.target_size, Image.Resampling.LANCZOS)
                        except AttributeError:
                            # å›é€€åˆ°æ—§ç‰ˆæœ¬å¸¸é‡
                            pil_image = pil_image.resize(self.target_size, Image.LANCZOS)
                        
                        # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶å½’ä¸€åŒ–
                        frame_array = np.array(pil_image, dtype=np.float32) / 255.0
                        
                        # è½¬æ¢ä¸ºCHWæ ¼å¼ [3, H, W]
                        frame_array = np.transpose(frame_array, (2, 0, 1))
                        
                        frames.append(frame_array)
                        successful_frames += 1
                        
                    except (IndexError, Exception) as e:
                        print(f"   âš ï¸  è¯»å–å¸§ {idx} å¤±è´¥: {e}")
                        # ç”¨é»‘å¸§å¡«å……
                        black_frame = np.zeros((3, *self.target_size), dtype=np.float32)
                        frames.append(black_frame)
                
                reader.close()
                
                print(f"   âœ… æˆåŠŸè¯»å– {successful_frames}/{self.num_frames} å¸§")
                
                # è½¬æ¢ä¸º [1, num_frames, 3, H, W]
                frames_tensor = torch.tensor(np.array(frames)).unsqueeze(0)
                return frames_tensor
                
            except Exception as e:
                print(f"   âŒ imageioè¯»å–å¤±è´¥: {e}")
                # å°è¯•å¤‡é€‰æ–¹æ³•
                return self._extract_frames_alternative(video_path)
            
        except Exception as e:
            raise ValueError(f"è§†é¢‘è¯»å–å¤±è´¥: {e}")
    
    def _fix_double_extension(self, video_path):
        """å°è¯•ä¿®å¤åŒæ‰©å±•åé—®é¢˜"""
        # æ£€æŸ¥æ˜¯å¦æ˜¯åŒæ‰©å±•åé—®é¢˜
        basename = os.path.basename(video_path)
        if basename.count('.') > 1:
            # å°è¯•ç§»é™¤é‡å¤çš„æ‰©å±•å
            name_parts = basename.split('.')
            # ä¿ç•™æ–‡ä»¶åå’Œæœ€åä¸€ä¸ªæ‰©å±•å
            fixed_name = '.'.join(name_parts[:-2]) + '.' + name_parts[-1]
            fixed_path = os.path.join(os.path.dirname(video_path), fixed_name)
            
            # åŒæ—¶å°è¯•å…¶ä»–å¯èƒ½çš„ä¿®å¤
            possible_fixes = [
                fixed_path,
                video_path.replace('.mp4.mp4', '.mp4'),  # ç›´æ¥æ›¿æ¢åŒæ‰©å±•å
                os.path.join(os.path.dirname(video_path), 'test_video.mp4')  # å°è¯•ç®€å•åç§°
            ]
            
            for fix in possible_fixes:
                if os.path.exists(fix):
                    return fix
        
        return None
    
    def _extract_frames_alternative(self, video_path):
        """å¤‡é€‰è§†é¢‘è¯»å–æ–¹æ³•"""
        print("   ğŸ”„ å°è¯•å¤‡é€‰è§†é¢‘è¯»å–æ–¹æ³•...")
        
        try:
            # æ–¹æ³•1: ä½¿ç”¨imageio v3 API (å¦‚æœå¯ç”¨)
            try:
                import imageio.v3 as iio
                frames = iio.imread(video_path, index=None)  # è¯»å–æ‰€æœ‰å¸§
                print(f"   âœ… ä½¿ç”¨imageio v3 APIæˆåŠŸè¯»å– {len(frames)} å¸§")
                
                # å‡åŒ€é‡‡æ ·
                frame_indices = np.linspace(0, len(frames)-1, self.num_frames, dtype=int)
                sampled_frames = []
                
                for idx in frame_indices:
                    frame = frames[idx]
                    pil_image = Image.fromarray(frame)
                    
                    # è°ƒæ•´å°ºå¯¸
                    try:
                        pil_image = pil_image.resize(self.target_size, Image.Resampling.LANCZOS)
                    except AttributeError:
                        pil_image = pil_image.resize(self.target_size, Image.LANCZOS)
                    
                    frame_array = np.array(pil_image, dtype=np.float32) / 255.0
                    frame_array = np.transpose(frame_array, (2, 0, 1))
                    sampled_frames.append(frame_array)
                
                frames_tensor = torch.tensor(np.array(sampled_frames)).unsqueeze(0)
                return frames_tensor
                
            except Exception as e:
                print(f"   âŒ imageio v3 APIä¹Ÿå¤±è´¥: {e}")
                
            # æ–¹æ³•2: æ£€æŸ¥è§†é¢‘ç¼–ç å¹¶å°è¯•è½¬æ¢
            print("   ğŸ”„ æ£€æŸ¥è§†é¢‘ç¼–ç ä¿¡æ¯...")
            self._check_video_codec(video_path)
            
            raise ValueError("æ‰€æœ‰è§†é¢‘è¯»å–æ–¹æ³•éƒ½å¤±è´¥äº†")
            
        except Exception as e:
            raise ValueError(f"å¤‡é€‰è§†é¢‘è¯»å–æ–¹æ³•å¤±è´¥: {e}")
    
    def _check_video_codec(self, video_path):
        """æ£€æŸ¥è§†é¢‘ç¼–ç ä¿¡æ¯"""
        try:
            # ä½¿ç”¨FFmpegæ£€æŸ¥è§†é¢‘ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            result = subprocess.run([
                'ffprobe', '-v', 'error', '-select_streams', 'v:0',
                '-show_entries', 'stream=codec_name,width,height,r_frame_rate,duration',
                '-of', 'default=noprint_wrappers=1:nokey=1', video_path
            ], capture_output=True, text=True, timeout=10)
            
            if result.returncode == 0:
                info = result.stdout.strip().split('\n')
                print(f"   ğŸ“Š FFprobeè§†é¢‘ä¿¡æ¯:")
                print(f"      ç¼–ç å™¨: {info[0] if len(info) > 0 else 'æœªçŸ¥'}")
                print(f"      å®½åº¦: {info[1] if len(info) > 1 else 'æœªçŸ¥'}")
                print(f"      é«˜åº¦: {info[2] if len(info) > 2 else 'æœªçŸ¥'}")
                print(f"      å¸§ç‡: {info[3] if len(info) > 3 else 'æœªçŸ¥'}")
                print(f"      æ—¶é•¿: {info[4] if len(info) > 4 else 'æœªçŸ¥'}")
            else:
                print("   â„¹ï¸  FFprobeä¸å¯ç”¨æˆ–è§†é¢‘æ ¼å¼ä¸æ”¯æŒ")
                
        except Exception as e:
            print(f"   â„¹ï¸  æ— æ³•è·å–è§†é¢‘ç¼–ç ä¿¡æ¯: {e}")


class VisualBackbone(nn.Module):
    def __init__(self, backbone_type="resnet50", hidden_dim=256):
        super().__init__()
        self.hidden_dim = hidden_dim
        
        # ä½¿ç”¨ResNet50 - å…¼å®¹ä¸åŒtorchvisionç‰ˆæœ¬
        try:
            # ä¼˜å…ˆå°è¯•æ–°ç‰ˆæœ¬API
            weights = models.ResNet50_Weights.IMAGENET1K_V1
            resnet = models.resnet50(weights=weights)
            print("   ğŸ”§ ä½¿ç”¨æ–°ç‰ˆæœ¬torchvision API")
        except AttributeError:
            # å›é€€åˆ°æ—§ç‰ˆæœ¬API
            try:
                resnet = models.resnet50(pretrained=True)
                print("   ğŸ”§ ä½¿ç”¨æ—§ç‰ˆæœ¬torchvision API")
            except:
                # æœ€ç»ˆå¤‡ç”¨æ–¹æ¡ˆ
                resnet = models.resnet50()
                print("   âš ï¸  ä½¿ç”¨æ— é¢„è®­ç»ƒæƒé‡çš„ResNet50")
        
        self.conv1 = resnet.conv1
        self.bn1 = resnet.bn1
        self.relu = resnet.relu
        self.maxpool = resnet.maxpool
        self.layer1 = resnet.layer1  # 1/4
        self.layer2 = resnet.layer2  # 1/8  
        self.layer3 = resnet.layer3  # 1/16
        self.layer4 = resnet.layer4  # 1/32
        
        # ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œ
        self.fpn = self._build_fpn([256, 512, 1024, 2048], hidden_dim)
    
    def _build_fpn(self, in_channels_list, out_channels):
        layers = nn.ModuleDict()
        for i, in_channels in enumerate(in_channels_list):
            layers[f'c{i+1}'] = nn.Conv2d(in_channels, out_channels, 1)
        return layers
    
    def forward(self, x):
        batch_size, num_frames = x.shape[0], x.shape[1]
        all_frame_features = []
        
        for t in range(num_frames):
            frame = x[:, t]  # [batch_size, 3, H, W]
            
            # ç‰¹å¾æå–
            c1 = self.relu(self.bn1(self.conv1(frame)))
            c1 = self.maxpool(c1)
            
            c2 = self.layer1(c1)  # [batch, 256, H/4, W/4]
            c3 = self.layer2(c2)  # [batch, 512, H/8, W/8]
            c4 = self.layer3(c3)  # [batch, 1024, H/16, W/16]
            c5 = self.layer4(c4)  # [batch, 2048, H/32, W/32]
            
            # FPNç»Ÿä¸€ç»´åº¦
            p2 = self.fpn['c1'](c2)  # [batch, hidden_dim, H/4, W/4]
            p3 = self.fpn['c2'](c3)  # [batch, hidden_dim, H/8, W/8]
            p4 = self.fpn['c3'](c4)  # [batch, hidden_dim, H/16, W/16]
            p5 = self.fpn['c4'](c5)  # [batch, hidden_dim, H/32, W/32]
            
            frame_features = [p2, p3, p4, p5]
            all_frame_features.append(frame_features)
        
        # é‡ç»„ä¸ºå¤šå°ºåº¦ç‰¹å¾
        multi_scale_features = []
        for scale_idx in range(4):
            scale_features = []
            for t in range(num_frames):
                scale_features.append(all_frame_features[t][scale_idx])
            scale_tensor = torch.stack(scale_features, dim=1)
            multi_scale_features.append(scale_tensor)
        
        return multi_scale_features


class DepthPredictor(nn.Module):
    def __init__(self, hidden_dim=256):
        super().__init__()
        self.hidden_dim = hidden_dim
        
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(128, 256, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(256, hidden_dim, 3, padding=1),
            nn.ReLU()
        )
        
        self.adapter = nn.Conv2d(hidden_dim, hidden_dim, 1)
    
    def forward(self, x):
        batch_size, num_frames = x.shape[0], x.shape[1]
        depth_features = []
        
        for t in range(num_frames):
            frame = x[:, t]  # [batch_size, 3, H, W]
            depth_feat = self.encoder(frame)  # [batch_size, hidden_dim, H/8, W/8]
            depth_feat = self.adapter(depth_feat)
            depth_features.append(depth_feat)
        
        depth_tensor = torch.stack(depth_features, dim=1)
        return depth_tensor


# ============================================================================
# ä¸»ç¨‹åº - é’ˆå¯¹æ‚¨çš„å…·ä½“æ–‡ä»¶è·¯å¾„
# ============================================================================

def main():
    # ä½¿ç”¨æ‚¨æä¾›çš„å‡†ç¡®æ–‡ä»¶è·¯å¾„
    video_path = r"C:\Users\lenovo\Desktop\äººå·¥æ™ºèƒ½\test_video.mp4.mp4"
    
    print("ğŸš€ è§†é¢‘ç‰¹å¾æå–å™¨ - é’ˆå¯¹åŒæ‰©å±•åæ–‡ä»¶")
    print("=" * 60)
    print(f"ğŸ¯ ç›®æ ‡è§†é¢‘: {video_path}")
    print("=" * 60)
    
    # è¯¦ç»†çš„ç¯å¢ƒæ£€æŸ¥
    print("\nğŸ” ç¯å¢ƒæ£€æŸ¥:")
    print(f"  Pythonç‰ˆæœ¬: {sys.version}")
    print(f"  å·¥ä½œç›®å½•: {os.getcwd()}")
    
    # æ£€æŸ¥ä¾èµ–åº“ç‰ˆæœ¬
    try:
        import imageio
        print(f"  imageioç‰ˆæœ¬: {imageio.__version__}")
    except:
        print("  âŒ imageioæœªæ­£ç¡®å®‰è£…")
    
    try:
        from PIL import Image
        print(f"  Pillowç‰ˆæœ¬: {Image.__version__}")
    except:
        print("  âŒ Pillowæœªæ­£ç¡®å®‰è£…")
    
    # è¯¦ç»†æ£€æŸ¥è§†é¢‘æ–‡ä»¶
    print(f"\nğŸ“ è§†é¢‘æ–‡ä»¶æ£€æŸ¥:")
    if os.path.exists(video_path):
        print(f"  âœ… æ–‡ä»¶å­˜åœ¨: {video_path}")
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        file_stats = os.stat(video_path)
        file_size = file_stats.st_size
        print(f"  ğŸ“ æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚ ({file_size / (1024*1024):.2f} MB)")
        
        # æ£€æŸ¥æ–‡ä»¶æƒé™
        if os.access(video_path, os.R_OK):
            print("  âœ… æœ‰è¯»å–æƒé™")
        else:
            print("  âŒ æ²¡æœ‰è¯»å–æƒé™")
            
        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        _, ext = os.path.splitext(video_path)
        print(f"  ğŸ“„ æ–‡ä»¶æ‰©å±•å: {ext}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯åŒæ‰©å±•å
        basename = os.path.basename(video_path)
        if basename.count('.') > 1:
            print(f"  âš ï¸  æ£€æµ‹åˆ°åŒæ‰©å±•å: {basename}")
            print(f"  ğŸ’¡ å»ºè®®é‡å‘½åä¸º: {basename.replace('.mp4.mp4', '.mp4')}")
        
        # æ”¯æŒçš„è§†é¢‘æ ¼å¼
        supported_formats = ['.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv', '.webm']
        if ext.lower() in supported_formats:
            print(f"  âœ… æ–‡ä»¶æ ¼å¼æ”¯æŒ")
        else:
            print(f"  âš ï¸  æ–‡ä»¶æ ¼å¼å¯èƒ½ä¸æ”¯æŒï¼Œæ”¯æŒçš„æ ¼å¼: {', '.join(supported_formats)}")
        
    else:
        print(f"  âŒ æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        print("  ğŸ’¡ å°è¯•è‡ªåŠ¨æŸ¥æ‰¾å¯èƒ½çš„æ–‡ä»¶...")
        
        # å°è¯•æŸ¥æ‰¾å¯èƒ½çš„æ–‡ä»¶
        video_dir = r"C:\Users\lenovo\Desktop\äººå·¥æ™ºèƒ½"
        if os.path.exists(video_dir):
            print(f"  ğŸ“‚ æ‰«æç›®å½•: {video_dir}")
            for file in os.listdir(video_dir):
                if 'test_video' in file.lower() and any(file.lower().endswith(ext) for ext in ['.mp4', '.avi', '.mov']):
                    print(f"  ğŸ” æ‰¾åˆ°å¯èƒ½çš„ç›®æ ‡æ–‡ä»¶: {file}")
                    full_path = os.path.join(video_dir, file)
                    print(f"  ğŸ’¡ å°è¯•ä½¿ç”¨: {full_path}")
                    video_path = full_path
                    break
        
        if not os.path.exists(video_path):
            print("  âŒ æœªæ‰¾åˆ°åˆé€‚çš„è§†é¢‘æ–‡ä»¶")
            return
    
    # å°è¯•åˆå§‹åŒ–ç‰¹å¾æå–å™¨
    try:
        print(f"\nğŸ¯ åˆå§‹åŒ–è§†é¢‘ç‰¹å¾æå–å™¨...")
        feature_extractor = VideoFeatureExtractor(
            target_size=(224, 224),
            num_frames=30,
            hidden_dim=256,
            device='cpu'
        )
        
        # å°è¯•æå–ç‰¹å¾
        print(f"\nğŸ¬ å¼€å§‹æå–è§†é¢‘ç‰¹å¾...")
        features = feature_extractor.extract_features(video_path)
        
        # æ‰“å°æå–ç»“æœæ‘˜è¦
        print("\nğŸ“Š ç‰¹å¾æå–æ‘˜è¦:")
        print(f"   è§†é¢‘æ–‡ä»¶: {os.path.basename(video_path)}")
        print(f"   è§†é¢‘å¸§æ•°: {features['num_frames']}")
        print(f"   è§†è§‰ç‰¹å¾å°ºåº¦æ•°: {len(features['visual_features'])}")
        print(f"   æ·±åº¦ç‰¹å¾å½¢çŠ¶: {features['depth_features'].shape}")
        
        # è‡ªåŠ¨ä¿å­˜ç‰¹å¾
        save_features(features, video_path)
        
        print("\nâœ… ç‰¹å¾æå–å®Œæˆå¹¶å·²ä¿å­˜!")
        
    except Exception as e:
        print(f"\nâŒ ç‰¹å¾æå–å¤±è´¥: {e}")
        print(f"\nğŸ’¡ è§£å†³æ–¹æ¡ˆå»ºè®®:")
        print("1. é‡å‘½åæ–‡ä»¶ï¼Œç§»é™¤é‡å¤çš„æ‰©å±•å")
        print("2. æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦æŸå")
        print("3. å®‰è£…FFmpeg: pip install imageio-ffmpeg")
        print("4. å°è¯•ä½¿ç”¨å…¶ä»–è§†é¢‘æ–‡ä»¶")


def save_features(features, video_path):
    """ä¿å­˜æå–çš„ç‰¹å¾åˆ°æ–‡ä»¶"""
    try:
        # åˆ›å»ºä¿å­˜ç›®å½•
        output_dir = "extracted_features"
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        video_name = os.path.splitext(os.path.basename(video_path))[0]
        # å¦‚æœæœ‰å¤šé‡æ‰©å±•åï¼Œåªå–ç¬¬ä¸€éƒ¨åˆ†ä½œä¸ºåç§°
        if '.' in video_name:
            video_name = video_name.split('.')[0]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{video_name}_features_{timestamp}.pt"
        filepath = os.path.join(output_dir, filename)
        
        # ä¿å­˜ç‰¹å¾
        torch.save(features, filepath)
        print(f"ğŸ’¾ ç‰¹å¾å·²ä¿å­˜åˆ°: {filepath}")
        
    except Exception as e:
        print(f"âŒ ç‰¹å¾ä¿å­˜å¤±è´¥: {e}")


if __name__ == "__main__":
    main()